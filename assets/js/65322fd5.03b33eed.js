"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[226],{5680:(e,n,t)=>{t.d(n,{xA:()=>u,yg:()=>g});var i=t(6540);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function p(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,i,r=function(e,n){if(null==e)return{};var t,i,r={},a=Object.keys(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var o=i.createContext({}),s=function(e){var n=i.useContext(o),t=n;return e&&(t="function"==typeof e?e(n):p(p({},n),e)),t},u=function(e){var n=s(e.components);return i.createElement(o.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},m=i.forwardRef((function(e,n){var t=e.components,r=e.mdxType,a=e.originalType,o=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),m=s(t),g=r,d=m["".concat(o,".").concat(g)]||m[g]||c[g]||a;return t?i.createElement(d,p(p({ref:n},u),{},{components:t})):i.createElement(d,p({ref:n},u))}));function g(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var a=t.length,p=new Array(a);p[0]=m;var l={};for(var o in n)hasOwnProperty.call(n,o)&&(l[o]=n[o]);l.originalType=e,l.mdxType="string"==typeof e?e:r,p[1]=l;for(var s=2;s<a;s++)p[s]=t[s];return i.createElement.apply(null,p)}return i.createElement.apply(null,t)}m.displayName="MDXCreateElement"},4398:(e,n,t)=>{t.r(n),t.d(n,{contentTitle:()=>p,default:()=>u,frontMatter:()=>a,metadata:()=>l,toc:()=>o});var i=t(8168),r=(t(6540),t(5680));const a={},p="Tune - AzureML pipeline",l={unversionedId:"Examples/Tune-AzureML-pipeline",id:"Examples/Tune-AzureML-pipeline",isDocsHomePage:!1,title:"Tune - AzureML pipeline",description:"This example uses flaml to tune an Azure ML pipeline that fits a lightgbm classifier on the sklearn breast cancer dataset.",source:"@site/docs/Examples/Tune-AzureML-pipeline.md",sourceDirName:"Examples",slug:"/Examples/Tune-AzureML-pipeline",permalink:"/FLAML/docs/Examples/Tune-AzureML-pipeline",editUrl:"https://github.com/microsoft/FLAML/edit/main/website/docs/Examples/Tune-AzureML-pipeline.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Integrate - Spark",permalink:"/FLAML/docs/Examples/Integrate - Spark"},next:{title:"Tune - HuggingFace",permalink:"/FLAML/docs/Examples/Tune-HuggingFace"}},o=[{value:"Prepare for tuning",id:"prepare-for-tuning",children:[{value:"Requirements",id:"requirements",children:[],level:3},{value:"Azure ML training pipeline",id:"azure-ml-training-pipeline",children:[],level:3},{value:"Data",id:"data",children:[],level:3},{value:"Configurations for the pipeline",id:"configurations-for-the-pipeline",children:[],level:3},{value:"Define and submit the pipeline",id:"define-and-submit-the-pipeline",children:[],level:3}],level:2},{value:"Hyperparameter Optimization",id:"hyperparameter-optimization",children:[{value:"Set up the tune job",id:"set-up-the-tune-job",children:[],level:3},{value:"Interact with AzureML pipeline jobs",id:"interact-with-azureml-pipeline-jobs",children:[],level:3}],level:2}],s={toc:o};function u(e){let{components:n,...a}=e;return(0,r.yg)("wrapper",(0,i.A)({},s,a,{components:n,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"tune---azureml-pipeline"},"Tune - AzureML pipeline"),(0,r.yg)("p",null,"This example uses flaml to tune an Azure ML pipeline that fits a lightgbm classifier on the ",(0,r.yg)("a",{parentName:"p",href:"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)"},"sklearn breast cancer dataset"),".\nIf you already have an Azure ML pipeline, you can use the approach to tune your pipeline with flaml."),(0,r.yg)("h2",{id:"prepare-for-tuning"},"Prepare for tuning"),(0,r.yg)("h3",{id:"requirements"},"Requirements"),(0,r.yg)("p",null,"We recommend using conda or venv to create a virtual env to install the dependencies."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"# set up new conda environment\nconda create -n pipeline_tune python=3.8 pip=20.2 -y\nconda activate pipeline_tune\n\n# install azureml packages for runnig AzureML pipelines\npip install azureml-core==1.39.0\npip install azure-ml-component[notebooks]==0.9.10.post1\npip install azureml-dataset-runtime==1.39.0\n\n# install hydra-core for passing AzureML pipeline parameters\npip install hydra-core==1.1.1\n\n# install flaml\npip install flaml[blendsearch,ray]==1.0.9\n")),(0,r.yg)("h3",{id:"azure-ml-training-pipeline"},"Azure ML training pipeline"),(0,r.yg)("p",null,"Before we are ready for tuning, we must first have an Azure ML pipeline.\nIn this example, we use the following toy pipeline for illustration.\nThe pipeline consists of two steps: (1) data preparation and (2) model training."),(0,r.yg)("p",null,(0,r.yg)("img",{alt:"png",src:t(9060).A}),"."),(0,r.yg)("p",null,"The ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/microsoft/FLAML/tree/main/test/pipeline_tuning_example"},"code example")," discussed in the page is included in\n",(0,r.yg)("inlineCode",{parentName:"p"},"test/pipeline_tuning_example/"),".\nWe will use the relative path in the rest of the page."),(0,r.yg)("h3",{id:"data"},"Data"),(0,r.yg)("p",null,"The example data exsits in ",(0,r.yg)("inlineCode",{parentName:"p"},"data/data.csv"),".\nIt will be uploaded to AzureML workspace to be consumed by the training pipeline\nusing the following code."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},'Dataset.File.upload_directory(\n    src_dir=to_absolute_path(LOCAL_DIR / "data"),\n    target=(datastore, "classification_data"),\n    overwrite=True,\n)\n\ndataset = Dataset.File.from_files(path=(datastore, "classification_data"))\n')),(0,r.yg)("h3",{id:"configurations-for-the-pipeline"},"Configurations for the pipeline"),(0,r.yg)("p",null,"The pipeline configuration is defined in\n",(0,r.yg)("inlineCode",{parentName:"p"},"configs/train_config.yaml"),"."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"hydra:\n  searchpath:\n    - file://.\n\naml_config:\n  workspace_name: your_workspace_name\n  resource_group: your_resource_group\n  subscription_id: your_subscription_id\n  cpu_target: cpucluster\n\ntrain_config:\n  exp_name: sklearn_breast_cancer_classification\n  test_train_ratio: 0.4\n  learning_rate: 0.05\n  n_estimators: 50\n")),(0,r.yg)("h3",{id:"define-and-submit-the-pipeline"},"Define and submit the pipeline"),(0,r.yg)("p",null,"The pipeline was defined in\n",(0,r.yg)("inlineCode",{parentName:"p"},"submit_train_pipeline.py"),"."),(0,r.yg)("p",null,"To submit the pipeline, please specify your AzureML resources\nin the ",(0,r.yg)("inlineCode",{parentName:"p"},"configs/train_config.yaml")," and run"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"cd test/pipeline_tuning_example\npython submit_train_pipeline.py\n")),(0,r.yg)("p",null,"To get the pipeline ready for HPO, in the training step,\nwe need to log the metrics of interest to AzureML using"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},'run.log(f"{data_name}_{eval_name}", result)\n')),(0,r.yg)("h2",{id:"hyperparameter-optimization"},"Hyperparameter Optimization"),(0,r.yg)("p",null,"We are now ready to set up the HPO job for the AzureML pipeline, including:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"config the HPO job,"),(0,r.yg)("li",{parentName:"ul"},"set up the interaction between the HPO job and the training job.")),(0,r.yg)("p",null,"These two steps are done in ",(0,r.yg)("inlineCode",{parentName:"p"},"tuner/tuner_func.py"),"."),(0,r.yg)("h3",{id:"set-up-the-tune-job"},"Set up the tune job"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"tuner_func.tune_pipeline")," sets up the search space, metric to optimize, mode, etc."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},'def tune_pipeline(concurrent_run=1):\n    start_time = time.time()\n\n    # config the HPO job\n    search_space = {\n        "train_config.n_estimators": flaml.tune.randint(50, 200),\n        "train_config.learning_rate": flaml.tune.uniform(0.01, 0.5),\n    }\n\n    hp_metric = "eval_binary_error"\n    mode = "max"\n    num_samples = 2\n\n    if concurrent_run > 1:\n        import ray  # For parallel tuning\n\n        ray.init(num_cpus=concurrent_run)\n        use_ray = True\n    else:\n        use_ray = False\n\n    # launch the HPO job\n    analysis = flaml.tune.run(\n        run_with_config,\n        config=search_space,\n        metric=hp_metric,\n        mode=mode,\n        num_samples=num_samples,  # number of trials\n        use_ray=use_ray,\n    )\n\n    # get the best config\n    best_trial = analysis.get_best_trial(hp_metric, mode, "all")\n    metric = best_trial.metric_analysis[hp_metric][mode]\n    print(f"n_trials={len(analysis.trials)}")\n    print(f"time={time.time()-start_time}")\n    print(f"Best {hp_metric}: {metric:.4f}")\n    print(f"Best coonfiguration: {best_trial.config}")\n')),(0,r.yg)("h3",{id:"interact-with-azureml-pipeline-jobs"},"Interact with AzureML pipeline jobs"),(0,r.yg)("p",null,"The interaction between FLAML and AzureML pipeline jobs is in ",(0,r.yg)("inlineCode",{parentName:"p"},"tuner_func.run_with_config"),"."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},'def run_with_config(config: dict):\n    """Run the pipeline with a given config dict"""\n\n    # pass the hyperparameters to AzureML jobs by overwriting the config file.\n    overrides = [f"{key}={value}" for key, value in config.items()]\n\n    print(overrides)\n    run = submit_train_pipeline.build_and_submit_aml_pipeline(overrides)\n\n    print(run.get_portal_url())\n\n    # retrieving the metrics to optimize before the job completes.\n    stop = False\n    while not stop:\n        # get status\n        status = run._core_run.get_status()\n        print(f"status: {status}")\n\n        # get metrics\n        metrics = run._core_run.get_metrics(recursive=True)\n        if metrics:\n            run_metrics = list(metrics.values())\n\n            new_metric = run_metrics[0]["eval_binary_error"]\n\n            if type(new_metric) == list:\n                new_metric = new_metric[-1]\n\n            print(f"eval_binary_error: {new_metric}")\n\n            tune.report(eval_binary_error=new_metric)\n\n        time.sleep(5)\n\n        if status == "FAILED" or status == "Completed":\n            stop = True\n\n    print("The run is terminated.")\n    print(status)\n\n    return\n')),(0,r.yg)("p",null,"Overall, to tune the hyperparameters of the AzureML pipeline, run:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"# the training job will run remotely as an AzureML job in both choices\n# run the tuning job locally\npython submit_tune.py --local\n# run the tuning job remotely\npython submit_tune.py --remote --subscription_id <your subscription_id> --resource_group <your resource_group> --workspace <your workspace>\n")),(0,r.yg)("p",null,"The local option runs the ",(0,r.yg)("inlineCode",{parentName:"p"},"tuner/tuner_func.py")," in your local machine.\nThe remote option wraps up the ",(0,r.yg)("inlineCode",{parentName:"p"},"tuner/tuner_func.py")," as an AzureML component and\nstarts another AzureML job to tune the AzureML pipeline."))}u.isMDXComponent=!0},9060:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/AzureML_train_pipeline-77c000b130274a1795f005eb627cc616.png"}}]);