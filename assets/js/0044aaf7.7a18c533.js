"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5868],{5680:(e,t,r)=>{r.d(t,{xA:()=>b,yg:()=>c});var a=r(6540);function l(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function s(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){l(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function n(e,t){if(null==e)return{};var r,a,l=function(e,t){if(null==e)return{};var r,a,l={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(l[r]=e[r]);return l}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(l[r]=e[r])}return l}var m=a.createContext({}),i=function(e){var t=a.useContext(m),r=t;return e&&(r="function"==typeof e?e(t):s(s({},t),e)),r},b=function(e){var t=i(e.components);return a.createElement(m.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},g=a.forwardRef((function(e,t){var r=e.components,l=e.mdxType,o=e.originalType,m=e.parentName,b=n(e,["components","mdxType","originalType","parentName"]),g=i(r),c=l,f=g["".concat(m,".").concat(c)]||g[c]||u[c]||o;return r?a.createElement(f,s(s({ref:t},b),{},{components:r})):a.createElement(f,s({ref:t},b))}));function c(e,t){var r=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var o=r.length,s=new Array(o);s[0]=g;var n={};for(var m in t)hasOwnProperty.call(t,m)&&(n[m]=t[m]);n.originalType=e,n.mdxType="string"==typeof e?e:l,s[1]=n;for(var i=2;i<o;i++)s[i]=r[i];return a.createElement.apply(null,s)}return a.createElement.apply(null,r)}g.displayName="MDXCreateElement"},3639:(e,t,r)=>{r.r(t),r.d(t,{contentTitle:()=>s,default:()=>b,frontMatter:()=>o,metadata:()=>n,toc:()=>m});var a=r(8168),l=(r(6540),r(5680));const o={},s="AutoML - Rank",n={unversionedId:"Examples/AutoML-Rank",id:"Examples/AutoML-Rank",isDocsHomePage:!1,title:"AutoML - Rank",description:"Prerequisites",source:"@site/docs/Examples/AutoML-Rank.md",sourceDirName:"Examples",slug:"/Examples/AutoML-Rank",permalink:"/FLAML/docs/Examples/AutoML-Rank",editUrl:"https://github.com/microsoft/FLAML/edit/main/website/docs/Examples/AutoML-Rank.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"AutoML - NLP",permalink:"/FLAML/docs/Examples/AutoML-NLP"},next:{title:"AutoML - Regression",permalink:"/FLAML/docs/Examples/AutoML-Regression"}},m=[{value:"Prerequisites",id:"prerequisites",children:[],level:3},{value:"A simple learning-to-rank example",id:"a-simple-learning-to-rank-example",children:[{value:"Sample output",id:"sample-output",children:[],level:4}],level:3}],i={toc:m};function b(e){let{components:t,...r}=e;return(0,l.yg)("wrapper",(0,a.A)({},i,r,{components:t,mdxType:"MDXLayout"}),(0,l.yg)("h1",{id:"automl---rank"},"AutoML - Rank"),(0,l.yg)("h3",{id:"prerequisites"},"Prerequisites"),(0,l.yg)("p",null,"Install the ","[","automl","]"," option."),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-bash"},'pip install "flaml[automl]"\n')),(0,l.yg)("h3",{id:"a-simple-learning-to-rank-example"},"A simple learning-to-rank example"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-python"},'from sklearn.datasets import fetch_openml\nfrom flaml import AutoML\n\nX_train, y_train = fetch_openml(name="credit-g", return_X_y=True, as_frame=False)\ny_train = y_train.cat.codes\n# not a real learning to rank dataaset\ngroups = [200] * 4 + [100] * 2  # group counts\nautoml = AutoML()\nautoml.fit(\n    X_train,\n    y_train,\n    groups=groups,\n    task="rank",\n    time_budget=10,  # in seconds\n)\n')),(0,l.yg)("p",null,(0,l.yg)("strong",{parentName:"p"},"Note"),": You can access the best model's estimator using ",(0,l.yg)("inlineCode",{parentName:"p"},"automl.model.estimator"),"."),(0,l.yg)("h4",{id:"sample-output"},"Sample output"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre"},"[flaml.automl: 11-15 07:14:30] {1485} INFO - Data split method: group\n[flaml.automl: 11-15 07:14:30] {1489} INFO - Evaluation method: holdout\n[flaml.automl: 11-15 07:14:30] {1540} INFO - Minimizing error metric: 1-ndcg\n[flaml.automl: 11-15 07:14:30] {1577} INFO - List of ML learners in AutoML Run: ['lgbm', 'xgboost']\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 0, current learner lgbm\n[flaml.automl: 11-15 07:14:30] {1944} INFO - Estimated sufficient time budget=679s. Estimated necessary time budget=1s.\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.1s,  estimator lgbm's best error=0.0248,     best estimator lgbm's best error=0.0248\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 1, current learner lgbm\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.1s,  estimator lgbm's best error=0.0248,     best estimator lgbm's best error=0.0248\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 2, current learner lgbm\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.2s,  estimator lgbm's best error=0.0248,     best estimator lgbm's best error=0.0248\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 3, current learner lgbm\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.2s,  estimator lgbm's best error=0.0248,     best estimator lgbm's best error=0.0248\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 4, current learner xgboost\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.2s,  estimator xgboost's best error=0.0315,  best estimator lgbm's best error=0.0248\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 5, current learner xgboost\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.2s,  estimator xgboost's best error=0.0315,  best estimator lgbm's best error=0.0248\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 6, current learner lgbm\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.3s,  estimator lgbm's best error=0.0248,     best estimator lgbm's best error=0.0248\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 7, current learner lgbm\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.3s,  estimator lgbm's best error=0.0248,     best estimator lgbm's best error=0.0248\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 8, current learner xgboost\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.4s,  estimator xgboost's best error=0.0315,  best estimator lgbm's best error=0.0248\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 9, current learner xgboost\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.4s,  estimator xgboost's best error=0.0315,  best estimator lgbm's best error=0.0248\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 10, current learner xgboost\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.4s,  estimator xgboost's best error=0.0233,  best estimator xgboost's best error=0.0233\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 11, current learner xgboost\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.4s,  estimator xgboost's best error=0.0233,  best estimator xgboost's best error=0.0233\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 12, current learner xgboost\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.4s,  estimator xgboost's best error=0.0233,  best estimator xgboost's best error=0.0233\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 13, current learner xgboost\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.4s,  estimator xgboost's best error=0.0233,  best estimator xgboost's best error=0.0233\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 14, current learner lgbm\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.5s,  estimator lgbm's best error=0.0225,     best estimator lgbm's best error=0.0225\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 15, current learner xgboost\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.5s,  estimator xgboost's best error=0.0233,  best estimator lgbm's best error=0.0225\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 16, current learner lgbm\n[flaml.automl: 11-15 07:14:30] {2029} INFO -  at 0.5s,  estimator lgbm's best error=0.0225,     best estimator lgbm's best error=0.0225\n[flaml.automl: 11-15 07:14:30] {1826} INFO - iteration 17, current learner lgbm\n[flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.5s,  estimator lgbm's best error=0.0225,     best estimator lgbm's best error=0.0225\n[flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 18, current learner lgbm\n[flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.6s,  estimator lgbm's best error=0.0225,     best estimator lgbm's best error=0.0225\n[flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 19, current learner lgbm\n[flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.6s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201\n[flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 20, current learner lgbm\n[flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.6s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201\n[flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 21, current learner lgbm\n[flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.7s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201\n[flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 22, current learner lgbm\n[flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.7s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201\n[flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 23, current learner lgbm\n[flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.8s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201\n[flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 24, current learner lgbm\n[flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.8s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201\n[flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 25, current learner lgbm\n[flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.8s,  estimator lgbm's best error=0.0201,     best estimator lgbm's best error=0.0201\n[flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 26, current learner lgbm\n[flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.9s,  estimator lgbm's best error=0.0197,     best estimator lgbm's best error=0.0197\n[flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 27, current learner lgbm\n[flaml.automl: 11-15 07:14:31] {2029} INFO -  at 0.9s,  estimator lgbm's best error=0.0197,     best estimator lgbm's best error=0.0197\n[flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 28, current learner lgbm\n[flaml.automl: 11-15 07:14:31] {2029} INFO -  at 1.0s,  estimator lgbm's best error=0.0197,     best estimator lgbm's best error=0.0197\n[flaml.automl: 11-15 07:14:31] {1826} INFO - iteration 29, current learner lgbm\n[flaml.automl: 11-15 07:14:31] {2029} INFO -  at 1.0s,  estimator lgbm's best error=0.0197,     best estimator lgbm's best error=0.0197\n[flaml.automl: 11-15 07:14:31] {2242} INFO - retrain lgbm for 0.0s\n[flaml.automl: 11-15 07:14:31] {2247} INFO - retrained model: LGBMRanker(colsample_bytree=0.9852774042640857,\n           learning_rate=0.034918421933217675, max_bin=1023,\n           min_child_samples=22, n_estimators=6, num_leaves=23,\n           reg_alpha=0.0009765625, reg_lambda=21.505295697527654, verbose=-1)\n[flaml.automl: 11-15 07:14:31] {1608} INFO - fit succeeded\n[flaml.automl: 11-15 07:14:31] {1610} INFO - Time taken to find the best model: 0.8846545219421387\n[flaml.automl: 11-15 07:14:31] {1624} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n")))}b.isMDXComponent=!0}}]);